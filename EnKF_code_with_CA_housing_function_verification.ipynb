{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64022634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use some real life data, and make the codes as generic as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4054f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ac5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_data = fetch_california_housing()['data']\n",
    "y_data = fetch_california_housing()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019ce1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b9d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b207d887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 8) (4128, 8) (16512,) (4128,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c0f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model to optimize - here we go with the model that Ved has used, we can change this later.\n",
    "\n",
    "# Fit a model with 256 input features, 32 neurons in the hidden layer and a single output layer\n",
    "\n",
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12bd0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the initial ensembles. \n",
    "\n",
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b44c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function seems to get the final predicted values with the initial ensemble values obtained from the MVN distribution,\n",
    "# for all ensembles at once\n",
    "\n",
    "def get_targets_with_weights(batch_data, initial_ensembles, weights_ann_1, h1, size_ens): \n",
    "\n",
    "    # dimensions of the target\n",
    "    target_dim = 1\n",
    "    \n",
    "    # total number of weights in the hidden layer (without the bias)\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    # use the drawn values from the initial ensembles for just the hidden weights and arange them in the shape\n",
    "    # J * features(denote this by p) * hidden_neurons(this by h) - where J is the number of ensembles\n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    # This will have the shape (J,n, h) after the matrix multiplication - where n is the batch size, J- no.of ensembles,\n",
    "    # h - no.of hidden neurons\n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    # bias will have the same dimensions as the number of hidden neurons. The shape of this will be (J, 1, h) \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "    # the returned output will have shape (J, n, h) (The bias values (1, h) gets added to each of the (n, h) row-wise)\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    # moving to the last layer, the number of total weights associated with the prediction layer\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    # arange the weights from the initial ensembles in the form (J, h, target_dim)\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "    # This will have the dimensions (J,n,target_dim)\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "    # arange the biases from the initial ensembles - will have the shape (J, 1, target_dim, and target_dim is usually 1)\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "    # will have the shape (J, n, 1) (biases (1,1) will get added to each (n,1) row-wise)\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    # collapse the final output across the last column - will have shape (J,n)\n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    ## print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    # stack the weights of the output - has shape (J, n) and the initial ensembles - has shape (J, n_total_weights_of_network)\n",
    "    # The stack will have a shape of (J, n + n_total_weights_of_network)\n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51624d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if this function is needed - very similar to the get targets with weights function we wrote, this might be \n",
    "# necessary\n",
    "\n",
    "def forward_operation(data1, combined_ensembles , size_ens, samp_ann_params):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1,weights_ann_1, h1, size_ens = size_ens)\n",
    "    return(initial_ensembles1, data1_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f07f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to give the predicted values - this function might also be redundant\n",
    "def get_predictions(data1, initial_ensembles): \n",
    "    _, data1_out1 = forward_operation(data1, initial_ensembles, size_ens, samp_ann_params)\n",
    "    return data1_out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85fb3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, initial_ensembles):\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    _,G_u = forward_operation(data1,initial_ensembles, size_ens = size_ens, samp_ann_params=samp_ann_params)\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07dc6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u, samp_ann_params, size_ens): \n",
    "    # computes the differences all at once for all ensembles\n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    \n",
    "    # now for each ensemble compute the u thing times the g thing in te formula, and we have the C_u\n",
    "    c = np.zeros((samp_ann_params, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    # here c/size_ens is the C_u\n",
    "    return c/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c150c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This follows a similar form as the C_u computation\n",
    "def calculate_D_u(G_bar, G_u, size_ens): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "859c50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the updated ensemble \n",
    "def get_updated_ensemble(data1, initial_ensembles, y_train, gamma, size_ens):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, initial_ensembles)\n",
    "    # get C\n",
    "    C = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u, samp_ann_params, size_ens)\n",
    "    # Get D\n",
    "    D = calculate_D_u( G_bar, G_u, size_ens)\n",
    "    # Get Tau \n",
    "    Tau = gamma * np.identity(D.shape[0])\n",
    "    # Tau inverse\n",
    "    Tau_inv = np.linalg.inv(Tau)\n",
    "    # D and Tau\n",
    "    D_plus_cov = D + Tau_inv\n",
    "    # inv of above\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    # CD tau\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    # true - predicted\n",
    "    # and add some random noise in here\n",
    "    mean_res = np.zeros((G_u.shape[1],))\n",
    "    # the variance here can be considered as a hyperparameter\n",
    "    cov_res = 0.001*np.identity(G_u.shape[1])\n",
    "    mvn_samp_res = mvn(mean_res, cov_res)\n",
    "    G_noise = mvn_samp_res.rvs(G_u.shape[0])\n",
    "    right_quant = y_train.T.flatten().reshape(1,-1) + G_noise - G_u\n",
    "    # do mid times right\n",
    "    mid_times_right = mid_quant@right_quant.T\n",
    "    # get the updated ensemble values\n",
    "    updated_ensemble = (initial_ensembles + mid_times_right.T)\n",
    "    return updated_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4baf21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15720b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(catch_train, y_train1, idxes): \n",
    "    \n",
    "    data1 = catch_train[idxes,:]\n",
    "    \n",
    "    y_train = y_train1[idxes].reshape(-1,1)\n",
    "    \n",
    "    return data1, y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59eb6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_test(catch_test,y_test, size): \n",
    "    idxes = random.sample(range(0, catch_test.shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1 = catch_test[idxes,:]\n",
    "    y_test = y_test[idxes].reshape(-1,1)\n",
    "    return data1, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "833bb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "samp_ann = ann(32,X_train.shape[1],1)\n",
    "\n",
    "# generate initial ensembles\n",
    "n_weights = samp_ann.count_params()\n",
    "initial_ensembles_1 = generate_initial_ensembles(n_weights, 0.01, 100)\n",
    "\n",
    "# define other quantities\n",
    "gamma = 0.001\n",
    "threshold = 20\n",
    "batch_size = 2500\n",
    "size_ens = 100\n",
    "samp_ann_params = samp_ann.count_params()\n",
    "weights_ann_1 = samp_ann.get_weights()\n",
    "h1 = samp_ann.layers[1].output.shape[-1]\n",
    "size = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d3a51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(idx, print_true = True):\n",
    "    \n",
    "    train_rmse = []\n",
    "    test_rmse = []\n",
    "    \n",
    "    train_idxes = random.sample(range(0, X_train.shape[0]), k = X_train.shape[0])\n",
    "    \n",
    "    train_chunks = list(chunks(train_idxes, batch_size))\n",
    "    \n",
    "    best_rmse_train = 10000\n",
    "    \n",
    "    data1_train_all,  y_train_all = prepare_data_train(X_train, y_train, train_idxes)\n",
    "    \n",
    "    \n",
    "    initial_ensembles = initial_ensembles_1\n",
    "    patience = 0\n",
    "    \n",
    "    # This is whee the epoch is denoted\n",
    "    for i in range(0,300):\n",
    "        \n",
    "        train_chunks = random.sample(train_chunks, len(train_chunks))\n",
    "        \n",
    "        if print_true == True:\n",
    "            print(\"epoch number is \" +str(i))\n",
    "        \n",
    "        for chunk in (train_chunks):\n",
    "            # for train data\n",
    "            data1_train, y1_train = prepare_data_train(X_train, y_train, chunk)\n",
    "\n",
    "            initial_ensembles = get_updated_ensemble(data1_train, initial_ensembles, y1_train, gamma, size_ens = size_ens)\n",
    "        \n",
    "            G_u_train = get_predictions(data1_train_all, initial_ensembles)\n",
    "    \n",
    "            li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)    \n",
    "            ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)  \n",
    "    \n",
    "            width_train = ui_train - li_train\n",
    "            avg_width_train = width_train.mean(0)[0]\n",
    "    \n",
    "            ind_train = (y_train_all >= li_train) & (y_train_all <= ui_train)\n",
    "            coverage_train= ind_train.mean(0)[0]\n",
    "    \n",
    "            averaged_targets_train = G_u_train.mean(0).reshape(-1,1)\n",
    "            rmse_train = np.sqrt(((y_train_all - averaged_targets_train)**2).mean(0))[0]\n",
    "        \n",
    "            pearsonr_train = pearsonr(averaged_targets_train.reshape(averaged_targets_train.shape[0],), \n",
    "                                 y_train_all.reshape(y_train_all.shape[0],))\n",
    "        \n",
    "            r_train = pearsonr_train.statistic\n",
    "            \n",
    "            # for test data\n",
    "            data1_test, y1_test = prepare_data_test(X_test,y_test, size)\n",
    "    \n",
    "            G_u_test = get_predictions(data1_test, initial_ensembles)\n",
    "\n",
    "    \n",
    "            li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)     \n",
    "            ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)   \n",
    "    \n",
    "            width_test = ui_test - li_test\n",
    "            avg_width_test = width_test.mean(0)[0]\n",
    "    \n",
    "            ind_test = (y1_test >= li_test) & (y1_test <= ui_test)\n",
    "            coverage_test= ind_test.mean(0)[0]\n",
    "    \n",
    "            averaged_targets_test = G_u_test.mean(0).reshape(-1,1)\n",
    "            rmse_test = np.sqrt(((y1_test -averaged_targets_test)**2).mean(0))[0]  \n",
    "        \n",
    "            pearsonr_test = pearsonr(averaged_targets_test.reshape(averaged_targets_test.shape[0],), \n",
    "                                 y1_test.reshape(y1_test.shape[0],))\n",
    "        \n",
    "            r_test = pearsonr_test.statistic\n",
    "            \n",
    "            train_rmse.append(rmse_train)\n",
    "            \n",
    "            test_rmse.append(rmse_test)\n",
    "            \n",
    "            \n",
    "            if print_true == True:\n",
    "                print(\"Training Coverage, Widths, RMSE, and Pearson R\")\n",
    "                print(coverage_train, avg_width_train, rmse_train, r_train)\n",
    "                print(\"Testing Coverage, Widths, RMSE, and Pearson R\")\n",
    "                print(coverage_test, avg_width_test, rmse_test, r_test)\n",
    "            \n",
    "\n",
    "            if (rmse_train < best_rmse_train): \n",
    "                best_pearsonr_train = r_train \n",
    "                best_train_width_mean = avg_width_train.mean()\n",
    "                best_train_width = avg_width_train\n",
    "                # best_smiles_weight = w1.mean()\n",
    "                best_coverage_train = coverage_train\n",
    "                best_rmse_train = rmse_train\n",
    "                best_pearson_r = r_test\n",
    "                best_test_width = avg_width_test\n",
    "\n",
    "                best_coverage_test = coverage_test    \n",
    "                best_rmse_test = rmse_test\n",
    "                patience = 0\n",
    "                best_ensembles = initial_ensembles\n",
    "                \n",
    "                best_test_preds = averaged_targets_test\n",
    "                best_li = li_test\n",
    "                best_ui = ui_test\n",
    "                \n",
    "                best_residuals = (y_test -averaged_targets_test)\n",
    "            \n",
    "            else:\n",
    "                patience = patience + 1\n",
    "            \n",
    "            if print_true == True:\n",
    "                print(\"Patience is\")\n",
    "                print(patience)\n",
    "                print('\\n')\n",
    "        \n",
    "            if patience > threshold:\n",
    "                print(\"test_coverage\" + str(best_coverage_test), flush = True)\n",
    "                print(\"test_width\" + str(best_test_width), flush = True)\n",
    "                print(\"rmse_test\" + str(best_rmse_test), flush = True)\n",
    "                \n",
    "                return(best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, \n",
    "           best_rmse_test, best_pearson_r, best_ensembles, train_rmse, test_rmse,  best_test_preds, \n",
    "           best_li, best_ui, best_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a512fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 264.82570919199435 13.968452104184783 0.024598727862479106\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 261.18355657360524 13.41285556357376 0.02359733018832278\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 261.4260388498862 13.701001127397205 0.024677917420058055\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 257.7896430508467 13.154220472408776 0.02370870456711903\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 261.0981545334772 13.584625145293906 0.02474700776759591\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 257.46688156608946 13.041398295917563 0.023797977260010796\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 261.63732174221644 12.808142713070463 0.02492085438758681\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 258.0033237635947 12.294595055284685 0.02403263091134553\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 259.5928277704727 12.928108896250718 0.02506040112615186\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 255.99058717829323 12.406432251885645 0.024196624185908698\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 258.63432476592925 11.53067147233012 0.025361708095803376\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 255.0377171860993 11.064267298332755 0.02460170946098043\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 255.73199006275783 11.123239827177274 0.02559749592011418\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 252.19385083700251 10.671184714748634 0.02489469193708351\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 248.647962822769 10.949328049532642 0.025831214073464358\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 245.21565854041992 10.502185895890621 0.025173304285017628\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 244.48203638492095 10.807378600123823 0.026130374378175007\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 241.11287113030266 10.363422556940256 0.02552493204167621\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 239.0355016056993 10.482066750135878 0.026488929896133523\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 235.71897946280396 10.049453132880817 0.02594689687974831\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 235.7349073536097 10.176402852996873 0.02690274052211997\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 232.4152110836519 9.754571158785193 0.026429732497474497\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 232.22595460079478 9.948799595166117 0.027210729583853215\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 228.96262281985338 9.535293862055717 0.026790122187125218\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 233.59141841710365 9.599043245491531 0.027774135120526303\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 230.3322900604116 9.198765984631114 0.02744912117866229\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 231.37363774559248 9.03463177003898 0.028494778709741018\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 228.18199858326852 8.657122096587864 0.028297161188545694\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 228.97653872379865 8.476386407601078 0.029364273861404774\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 225.83549359336175 8.121667069964179 0.029315734847916147\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 225.58484894521743 8.07411210073693 0.03040815466797152\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 222.5022729786622 7.7358092939747305 0.030527739349837643\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 221.09644112466043 7.7484044153691105 0.031177807978521942\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 218.06800493599755 7.4235866866698625 0.03142174017947409\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 217.593917956613 7.371641004265769 0.032436230920045\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 214.60659031154128 7.062955265330904 0.03287188611803488\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 213.52972057650155 6.932724622879336 0.03405095367399675\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 210.57111177535708 6.642963833919082 0.03473306820177222\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 208.79188289988596 6.300772956920512 0.03633497576185135\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 205.9194203275072 6.0385442878044 0.03738431989367996\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 202.72968704915317 5.595581848629999 0.039547500966107686\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 199.9624003774796 5.364370187189985 0.04110959380965129\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 196.042459718208 5.004358359307976 0.043419533666133624\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 193.38424803688147 4.7995752892068495 0.04558556181898765\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 189.632839582944 4.42373411585463 0.04859191758381301\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 187.0743781385353 4.245102807838504 0.05156334589651817\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 183.50700753618457 3.816235576459175 0.05616026438969629\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 181.051631762499 3.6649866567729523 0.06028649553705893\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 178.43712537443864 3.493044416325159 0.06215300996082354\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 176.057284816999 3.3563735744509335 0.06718154817668384\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 172.9325609650962 3.04657186865454 0.07468059937134154\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 170.62701786181745 2.9308537406440744 0.0815191283083311\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 166.14614544812738 2.6761184152088893 0.09241980236262694\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 163.9310301151107 2.577822229402033 0.10178501948402369\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 157.1748433151394 2.666749409074768 0.10018553127926032\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 155.11313605628757 2.571341758840474 0.11039752243518472\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 151.83648073903512 2.2425057720472767 0.13854281959726084\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 149.85127434762674 2.165143892071831 0.15386653271193185\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 144.3349466232069 2.0820901220763206 0.17465049348257447\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 142.45110259338867 2.0125000795300694 0.1942006346752793\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 138.03374861760858 1.784214225124966 0.2859938678236132\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 136.2260376002918 1.7252511452485573 0.3162139308882918\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 131.6651882894484 1.5132086283306576 0.5412377705036364\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 129.9388560608498 1.4601533971407492 0.5783254698439638\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 126.25081038209022 1.3908184900290548 0.4213009050676252\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 124.58800365280779 1.3369787473792727 0.46947039864650353\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 120.27638581603395 1.3535220732488387 0.3212688678060349\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 118.68270489769145 1.2970769826967747 0.3651091780065927\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 115.9107927635048 1.3434848981971947 0.2982526425336434\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 114.369293144673 1.2856441698921166 0.340188862711184\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 111.64325695114718 1.3526447641507915 0.27314537009080325\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 110.13567291988343 1.2928335453204804 0.3124865654060529\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 107.91217356870024 1.4186151472143207 0.3275024357775228\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 106.43238702744712 1.3618049325371635 0.3703698351196791\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 105.17475187217318 1.4573082994728606 0.3765551403631867\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 103.72404448196525 1.4022276951769153 0.42160937537497734\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 101.79871686639702 1.5645389101412472 0.464311010835159\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 100.40014526164546 1.5118843679668084 0.509295032476135\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 97.16948945834189 1.70893817868737 0.5738245110349023\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 95.83475473620186 1.6579198201572107 0.6095231889874034\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 97.73761549497841 2.5646926668714447 0.26104341417614746\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 96.3681887290138 2.499118118580289 0.2855611458882613\n",
      "Patience is\n",
      "6\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 102.28230232706993 3.6771245222270235 0.14383168438605332\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 100.89017005870751 3.574245478450825 0.1570833795681651\n",
      "Patience is\n",
      "7\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 94.62410087928174 3.822218813003018 0.13988887983885948\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 93.32820307855847 3.7133643779567573 0.152601118713161\n",
      "Patience is\n",
      "8\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 86.77111107681246 3.9102938578895436 0.13937773712398854\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 85.56065056287804 3.7983523329683644 0.15192588089137637\n",
      "Patience is\n",
      "9\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9999394379844961 80.89352013817918 4.750059213432219 0.11305097774941805\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 79.8069291495098 4.605836868349881 0.12254715968325676\n",
      "Patience is\n",
      "10\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9999394379844961 75.79597648906277 5.399334133576019 0.10025155795025165\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 74.76057814215206 5.2273913119598525 0.1082313707476998\n",
      "Patience is\n",
      "11\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9998788759689923 67.6442217784013 5.43097418509433 0.10031756760698454\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 66.72120155670098 5.255237436869274 0.10830334788269469\n",
      "Patience is\n",
      "12\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9998183139534884 60.6024367033334 5.759318631584877 0.09621483371073895\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 59.82152813339163 5.568898690445166 0.10368127150393519\n",
      "Patience is\n",
      "13\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.999515503875969 49.03218288740069 6.133253695522896 0.09187060065946895\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9997577519379846 48.38304977377175 5.924113255809359 0.09883779437389595\n",
      "Patience is\n",
      "14\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9993943798449613 40.43207573703546 6.031818239897774 0.09431801377053707\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9997577519379846 39.87705661986286 5.823359192438595 0.10156497356730136\n",
      "Patience is\n",
      "15\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984253875968992 24.873431980794802 4.580545805660649 0.12479740149589913\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9987887596899225 24.59309870429038 4.429619588603999 0.13544305861830003\n",
      "Patience is\n",
      "16\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9940649224806202 21.88334254266112 1.9512584876320078 0.4175282845852534\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9956395348837209 21.56217492087144 1.8896210857095197 0.4481692570007138\n",
      "Patience is\n",
      "17\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9989098837209303 55.66515545513386 4.228405144055194 0.13903911900501997\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999031007751938 54.92102292927689 4.087036885290678 0.15116013415635282\n",
      "Patience is\n",
      "18\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9972747093023255 45.73446602550755 6.756118585626792 0.08921954895037276\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998062015503876 45.11696639708594 6.500611424106541 0.09570862702484104\n",
      "Patience is\n",
      "19\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.999515503875969 58.066368785991024 2.0552044299012877 0.33420810539864115\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999515503875969 57.27727347446452 1.9828453821689322 0.35897827149538786\n",
      "Patience is\n",
      "20\n",
      "\n",
      "\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9988493217054264 67.56866611958274 10.883409703285086 0.06435140599373532\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992732558139535 66.68554172595238 10.449948118400632 0.06777437349793342\n",
      "Patience is\n",
      "21\n",
      "\n",
      "\n",
      "test_coverage1.0\n",
      "test_width114.369293144673\n",
      "rmse_test1.2856441698921166\n",
      "Wall time: 15min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles, train_rmse, test_rmse, \\\n",
    "best_test_preds, best_li, best_ui, best_residuals = get_results(0, print_true = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gayarad",
   "language": "python",
   "name": "gayarad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
